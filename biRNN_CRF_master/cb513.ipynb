{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "%matplotlib nbagg\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework.ops import reset_default_graph\n",
    "\n",
    "import data\n",
    "import utils\n",
    "import custom_ops\n",
    "import conditional_random_fields as crf\n",
    "\n",
    "X_train = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chop_sequences(X, t, mask, length):\n",
    "    max_len = np.max(length)\n",
    "    return X[:, :max_len], t[:, :max_len], mask[:, :max_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import fully_connected, batch_norm\n",
    "\n",
    "reset_default_graph()\n",
    "\n",
    "# Defining model\n",
    "num_iterations = 3e4\n",
    "batch_size=64\n",
    "number_inputs=42\n",
    "number_outputs=8\n",
    "seq_len=None#100 # max 700\n",
    "learning_rate = 0.001\n",
    "\n",
    "X_input = tf.placeholder(tf.float32, shape=[None, None, number_inputs], name='X_input')\n",
    "X_length = tf.placeholder(tf.int32, shape=[None,], name='X_length')\n",
    "t_input = tf.placeholder(tf.int32, shape=[None, None], name='t_input')\n",
    "t_input_hot = tf.one_hot(t_input, number_outputs)\n",
    "t_mask = tf.placeholder(tf.float32, shape=[None, None], name='t_mask')\n",
    "\n",
    "num_units_encoder = 400\n",
    "num_units_l1 = 200\n",
    "num_units_l2 = 200\n",
    "\n",
    "l1 = fully_connected(X_input, num_units_l1)\n",
    "l1 = tf.concat(2, [X_input, l1])\n",
    "\n",
    "cell_fw = tf.nn.rnn_cell.GRUCell(num_units_encoder)\n",
    "cell_bw = tf.nn.rnn_cell.GRUCell(num_units_encoder)\n",
    "#enc_cell = tf.nn.rnn_cell.OutputProjectionWrapper(enc_cell, number_outputs)\n",
    "enc_outputs, _ = tf.nn.bidirectional_dynamic_rnn(cell_fw=cell_fw, cell_bw=cell_bw, inputs=l1,\n",
    "                                                 sequence_length=X_length, dtype=tf.float32)\n",
    "enc_outputs = tf.concat(2, enc_outputs)\n",
    "\n",
    "outputs = tf.reshape(enc_outputs, [-1, num_units_encoder*2])\n",
    "l2 = fully_connected(outputs, num_units_l2)\n",
    "l_f = fully_connected(l2, number_outputs, activation_fn=None)\n",
    "l_g = fully_connected(l2, number_outputs**2, activation_fn=None)\n",
    "\n",
    "batch_size_shp = tf.shape(enc_outputs)[0]\n",
    "seq_len_shp = tf.shape(enc_outputs)[1]\n",
    "l_f_reshape = tf.reshape(l_f, [batch_size_shp, seq_len_shp, number_outputs])\n",
    "\n",
    "l_g_reshape = tf.reshape(l_g, [batch_size_shp, seq_len_shp, number_outputs**2])\n",
    "l_g_reshape = tf.reshape(l_g_reshape, [batch_size_shp, seq_len_shp, number_outputs, number_outputs])\n",
    "\n",
    "f = l_f_reshape\n",
    "g = tf.slice(l_g_reshape, [0, 0, 0, 0], [-1, seq_len_shp-1, -1, -1])\n",
    "\n",
    "# doesnt work\n",
    "#g_prev = tf.slice(f, [0, 0, 0], [-1, seq_len_shp-1, -1])\n",
    "#g_nxt = tf.slice(f, [0, 1, 0], [-1, -1, -1])\n",
    "#g = tf.concat(2, [g_prev, g_nxt])\n",
    "#g = tf.reshape(g, [tf.shape(g)[0], tf.shape(g)[1], tf.shape(f)[2], tf.shape(f)[2]])\n",
    "\n",
    "# zeros\n",
    "#g = tf.zeros(tf.shape(g))\n",
    "\n",
    "nu_alp = crf.forward_pass(f, g, X_length)\n",
    "nu_bet = crf.backward_pass(f, g, X_length)\n",
    "\n",
    "prediction = crf.log_marginal(nu_alp, nu_bet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loss_and_acc():\n",
    "    # sequence_loss_tensor is a modification of TensorFlow's own sequence_to_sequence_loss\n",
    "    # TensorFlow's seq2seq loss works with a 2D list instead of a 3D tensors\n",
    "    loss = -crf.log_likelihood(t_input_hot, f, g, nu_alp, nu_bet, X_length)#custom_ops.sequence_loss(preds, t_input, t_mask)\n",
    "    # if you want regularization\n",
    "    #reg_scale = 0.00001\n",
    "    #regularize = tf.contrib.layers.l2_regularizer(reg_scale)\n",
    "    #params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "    #reg_term = sum([regularize(param) for param in params])\n",
    "    #loss += reg_term\n",
    "    # calculate accuracy\n",
    "    argmax = tf.to_int32(tf.argmax(prediction, 2))\n",
    "    correct = tf.to_float(tf.equal(argmax, t_input)) * t_mask\n",
    "    accuracy = tf.reduce_sum(correct) / tf.reduce_sum(t_mask)\n",
    "    return loss, accuracy, argmax\n",
    "\n",
    "loss, accuracy, predictions = loss_and_acc()\n",
    "\n",
    "# use lobal step to keep track of our iterations\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "# pick optimizer, try momentum or adadelta\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "# extract gradients for each variable\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "# add below for clipping by norm\n",
    "#gradients, variables = zip(*grads_and_vars)  # unzip list of tuples\n",
    "#clipped_gradients, global_norm = (\n",
    "#    tf.clip_by_global_norm(gradients, self.clip_norm) )\n",
    "#grads_and_vars = zip(clipped_gradients, variables)\n",
    "# apply gradients and make trainable function\n",
    "train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if X_train is None:\n",
    "    X_train, X_valid, t_train, t_valid, mask_train, mask_valid, length_train, length_valid, num_seq_train =\\\n",
    "    data.get_train(seq_len)\n",
    "    X_valid, t_valid, mask_valid = chop_sequences(X_valid, t_valid, mask_valid, length_valid)\n",
    "    print(\"X_train,\", X_train.shape, X_train.dtype)\n",
    "    print(\"t_train,\", t_train.shape, t_train.dtype)\n",
    "    print(\"mask_train,\", mask_train.shape, mask_train.dtype)\n",
    "    print(\"length_train,\", length_train.shape, length_train.dtype)\n",
    "    print(\"num_seq_train\", num_seq_train)\n",
    "    print(\"X_valid,\", X_valid.shape, X_valid.dtype)\n",
    "    print(\"t_valid,\", t_valid.shape, t_valid.dtype)\n",
    "    print(\"mask_valid,\", mask_valid.shape, mask_valid.dtype)\n",
    "    print(\"length_valid,\", length_valid.shape, length_valid.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# restricting memory usage, TensorFlow is greedy and will use all memory otherwise\n",
    "gpu_opts = tf.GPUOptions(per_process_gpu_memory_fraction=0.9)\n",
    "# initialize the Session\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_opts))\n",
    "\n",
    "# test train part\n",
    "sess.run(tf.global_variables_initializer())\n",
    "feed_dict = {X_input: X_valid, X_length: length_valid, t_input: t_valid,\n",
    "             t_mask: mask_valid}\n",
    "fetches = [f, g, loss]\n",
    "res = sess.run(fetches=fetches, feed_dict=feed_dict)\n",
    "print \"f\", res[0].shape\n",
    "print \"g\", res[1].shape\n",
    "#print \"y_i\", res[2][0].shape\n",
    "#print \"y_plus\", res[2][1].shape\n",
    "print \"log_likelihood\", res[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle_data(X, t, mask, length):\n",
    "    num_sequences = X.shape[0]\n",
    "    seq_names = np.arange(0,num_sequences)\n",
    "    np.random.shuffle(seq_names)\n",
    "    X = X_train[seq_names]\n",
    "    t = t[seq_names]\n",
    "    mask = mask[seq_names]\n",
    "    length = length[seq_names]\n",
    "    return X, t, mask, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# setting up running parameters\n",
    "val_interval = batch_size*10\n",
    "num_batches_per_epoch = X_train.shape[0] // batch_size\n",
    "samples_to_process = 1e5\n",
    "samples_processed = 0\n",
    "num_samples_per_epoch = int(num_seq_train/batch_size)\n",
    "samples_val = []\n",
    "costs, accs_val = [], []\n",
    "plt.figure()\n",
    "try:\n",
    "    while samples_processed < samples_to_process:\n",
    "        seq_names = np.arange(0,num_seq_train)\n",
    "        np.random.shuffle(seq_names)     \n",
    "        X_train = X_train[seq_names]\n",
    "        t_train = t_train[seq_names]\n",
    "        mask_train = mask_train[seq_names]\n",
    "        length_train = length_train[seq_names]\n",
    "        losses = []\n",
    "        accs = []\n",
    "        for i in range(num_batches_per_epoch):\n",
    "            idx = range(i*batch_size, (i+1)*batch_size)\n",
    "            x_batch_tr = X_train[idx]\n",
    "            t_batch_tr = t_train[idx]\n",
    "            mask_batch_tr = mask_train[idx]\n",
    "            length_batch_tr = length_train[idx]\n",
    "            x_batch_tr, t_batch_tr, mask_batch_tr = chop_sequences(x_batch_tr, t_batch_tr,\n",
    "                                                                   mask_batch_tr, length_batch_tr)\n",
    "            fetches_tr = [train_op, loss, accuracy]\n",
    "            # set up feed dict\n",
    "            feed_dict_tr = {X_input: x_batch_tr, X_length: length_batch_tr,\n",
    "                            t_input: t_batch_tr, t_mask: mask_batch_tr}\n",
    "            # run the model\n",
    "            res = tuple(sess.run(fetches=fetches_tr, feed_dict=feed_dict_tr))\n",
    "            _, batch_cost, batch_acc = res\n",
    "            losses.append(batch_cost)\n",
    "            accs.append(batch_acc)\n",
    "            samples_processed += batch_size\n",
    "        #print(\"samples_processed,\", samples_processed)\n",
    "        #print(\"batch_cost,\", batch_cost)\n",
    "        #if samples_processed % 1000 == 0: print batch_cost, batch_acc\n",
    "        #validation data\n",
    "        if True:#samples_processed % val_interval == 0:\n",
    "            print \"validating\"\n",
    "            fetches_val = [prediction]\n",
    "            feed_dict_val = {X_input: X_valid, X_length: length_valid,\n",
    "                             t_input: t_valid, t_mask: mask_valid}\n",
    "            predictions = sess.run(fetches=fetches_val, feed_dict=feed_dict_val)[0]\n",
    "            acc_valid = utils.proteins_acc(predictions, t_valid, mask_valid)\n",
    "            samples_val += [samples_processed]\n",
    "            accs_val += [acc_valid]\n",
    "            #print(\"validation_accs\", acc_val)\n",
    "            plt.plot(samples_val, accs_val, 'g-')\n",
    "            plt.ylabel('Validation Accuracy', fontsize=15)\n",
    "            plt.xlabel('Processed samples', fontsize=15)\n",
    "            plt.title('', fontsize=20)\n",
    "            plt.grid('on')\n",
    "            plt.savefig(\"out.png\")\n",
    "            display.display(display.Image(filename=\"out.png\"))\n",
    "            display.clear_output(wait=True)\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
